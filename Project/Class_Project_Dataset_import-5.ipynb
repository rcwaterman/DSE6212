{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def reduce_twitter_dataset(input_file, output_file, sample_size=50000):\n",
        "    \"\"\"\n",
        "    Reduces the Sentiment140 dataset while maintaining class balance.\n",
        "    Columns: 0: target, 1: id, 2: date, 3: flag, 4: user, 5: text\n",
        "    \"\"\"\n",
        "    cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "\n",
        "    print(\"Reading dataset in chunks...\")\n",
        "    # Using 'latin-1' encoding as this dataset often has special characters\n",
        "    chunks = pd.read_csv(input_file, encoding='latin-1', header=None, names=cols, chunksize=100000)\n",
        "\n",
        "    df_list = []\n",
        "    for chunk in chunks:\n",
        "        # Keep only necessary columns to save memory immediately\n",
        "        df_list.append(chunk[['sentiment', 'text']])\n",
        "\n",
        "    full_df = pd.concat(df_list)\n",
        "\n",
        "    # Stratified sampling: Ensure 50/50 split of Positive (4) and Negative (0)\n",
        "    # The dataset uses 0=negative, 2=neutral (rare), 4=positive\n",
        "    negative_tweets = full_df[full_df['sentiment'] == 0].sample(n=sample_size//2, random_state=42)\n",
        "    positive_tweets = full_df[full_df['sentiment'] == 4].sample(n=sample_size//2, random_state=42)\n",
        "\n",
        "    reduced_df = pd.concat([negative_tweets, positive_tweets]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    reduced_df.to_csv(output_file, index=False)\n",
        "    print(f\"Success! Reduced file saved to {output_file} with {len(reduced_df)} rows.\")\n",
        "\n",
        "# Usage\n",
        "# reduce_twitter_dataset('training.1600000.processed.noemoticon.csv', 'reduced_sentiment.csv')"
      ],
      "metadata": {
        "id": "yulDP-1fP62T"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}