{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tTOJs-jGAco"
      },
      "source": [
        "Objectives:   \n",
        "\t•\tIntroduce text vectorization methods.   \n",
        "\t•\tExplore preprocessing steps (stemming, lemmatization, etc.) and their impact on vectorization.    \n",
        "\t•\tCompare methods on the Reuters dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o7Ulqs4iFtYy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to\n",
            "[nltk_data]     C:\\Users\\water\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Dataset:\n",
            "                                                text  category\n",
            "0  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...     trade\n",
            "1  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...     grain\n",
            "2  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...     crude\n",
            "3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...      corn\n",
            "4  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...  palm-oil\n"
          ]
        }
      ],
      "source": [
        "# Load and explore the Reuters dataset using NLTK.\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "import pandas as pd\n",
        "\n",
        "# Download Reuters dataset\n",
        "nltk.download('reuters')\n",
        "\n",
        "# Load dataset into a DataFrame\n",
        "categories = reuters.categories()\n",
        "files = reuters.fileids()\n",
        "texts = [reuters.raw(fileid) for fileid in files[:500]]  # Limit to 500 documents for efficiency\n",
        "df = pd.DataFrame({'text': texts, 'category': [reuters.categories(fileid)[0] for fileid in files[:500]]})\n",
        "\n",
        "# Display sample\n",
        "print(\"Sample Dataset:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qNy4ePaGjnf"
      },
      "source": [
        "### Stemming & Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8CfL6JjnGgUC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\water\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\water\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Original Tokens: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U.S.-JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U.S.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many']\n",
            "\n",
            "Porter Stemmed: ['asian', 'export', 'fear', 'damag', 'from', 'u.s.-japan', 'rift', 'mount', 'trade', 'friction', 'between', 'the', 'u.s.', 'and', 'japan', 'ha', 'rais', 'fear', 'among', 'mani']\n",
            "\n",
            "Snowball Stemmed: ['asian', 'export', 'fear', 'damag', 'from', 'u.s.-japan', 'rift', 'mount', 'trade', 'friction', 'between', 'the', 'u.s.', 'and', 'japan', 'has', 'rais', 'fear', 'among', 'mani']\n",
            "\n",
            "Lemmatized: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U.S.-JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U.S.', 'And', 'Japan', 'ha', 'raised', 'fear', 'among', 'many']\n"
          ]
        }
      ],
      "source": [
        "# Compare Porter, Snowball, and WordNetLemmatizer.\n",
        "\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample text\n",
        "sample_text = df['text'][0]\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "# Stemming\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer('english')\n",
        "porter_stemmed = [porter.stem(token) for token in tokens]\n",
        "snowball_stemmed = [snowball.stem(token) for token in tokens]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "print(\"\\nOriginal Tokens:\", tokens[:20])\n",
        "print(\"\\nPorter Stemmed:\", porter_stemmed[:20])\n",
        "print(\"\\nSnowball Stemmed:\", snowball_stemmed[:20])\n",
        "print(\"\\nLemmatized:\", lemmatized[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQmHEqYcG6zl"
      },
      "source": [
        "#### Handling Special Characters, Numbers and URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vL2qz3hHHAVJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cleaned Text: ASIAN EXPORTERS FEAR DAMAGE FROM USJAPAN RIFT\n",
            "  Mounting trade friction between the\n",
            "  US And Japan has raised fears among many of Asias exporting\n",
            "  nations that the row could inflict farreaching econo\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned_text = preprocess_text(sample_text)\n",
        "print(\"\\nCleaned Text:\", cleaned_text[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjX31bfEHLOx"
      },
      "source": [
        "### Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPk_fmCKHPc8"
      },
      "source": [
        "#### One Hot Encoding (OHE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rb7oFfIxHSeD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "One-Hot Encoded Shape: (10, 2251)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# One-Hot Encoding Example\n",
        "corpus = df['text'][:10]\n",
        "\n",
        "# Use CountVectorizer to create a vocabulary and transform the text into a document-term matrix\n",
        "vectorizer = CountVectorizer()\n",
        "document_term_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Use OneHotEncoder to encode the document-term matrix\n",
        "encoder = OneHotEncoder(sparse_output=False) # sparse_output=False for a dense array\n",
        "one_hot_encoded = encoder.fit_transform(document_term_matrix.toarray()) # Convert to dense array\n",
        "\n",
        "print(\"\\nOne-Hot Encoded Shape:\", one_hot_encoded.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wifmt2yIsxC"
      },
      "source": [
        "#### Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0rOAwNJSIsaL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bag of Words Shape: (10, 100)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Bag of Words\n",
        "bow_vectorizer = CountVectorizer(max_features=100)\n",
        "bow_matrix = bow_vectorizer.fit_transform(df['text'][:10])\n",
        "\n",
        "print(\"\\nBag of Words Shape:\", bow_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA0gSYhqI0vJ"
      },
      "source": [
        "#### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oE8lAqCYI1Od"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TF-IDF Shape: (10, 100)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'][:10])\n",
        "\n",
        "print(\"\\nTF-IDF Shape:\", tfidf_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKutmVE7I9jp"
      },
      "source": [
        "### Benchmarking and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GUxsjuP1JAyo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bag of Words Sparsity: 61.6 %\n",
            "TF-IDF Sparsity: 61.6 %\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_sparsity(matrix):\n",
        "    \"\"\"\n",
        "    Calculates sparsity percentage of a given matrix.\n",
        "    :param matrix: Sparse matrix or array\n",
        "    :return: Sparsity as a percentage\n",
        "    \"\"\"\n",
        "    # Convert the matrix to a dense array if it's sparse\n",
        "    if hasattr(matrix, 'toarray'):\n",
        "        matrix = matrix.toarray()\n",
        "    total_elements = np.prod(matrix.shape)\n",
        "    zero_elements = np.count_nonzero(matrix == 0)\n",
        "    sparsity = (zero_elements / total_elements) * 100\n",
        "    return sparsity\n",
        "\n",
        "# Calculate sparsity for Bag of Words and TF-IDF matrices\n",
        "print(\"\\nBag of Words Sparsity:\", calculate_sparsity(bow_matrix), \"%\")\n",
        "print(\"TF-IDF Sparsity:\", calculate_sparsity(tfidf_matrix), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl3CqrboKHv6"
      },
      "source": [
        "Reflection Questions    \n",
        "\t1. How does stemming or lemmatization affect the vocabulary size for vectorization?   \n",
        "\t2. What trade-offs exist between Bag of Words and TF-IDF in terms of dimensionality and interpretability?   \n",
        "\t3. Why might preprocessing steps (e.g., removing special characters or numbers) influence model performance?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DSE6212",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
