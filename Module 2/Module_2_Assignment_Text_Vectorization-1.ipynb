{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 2 Assignment: Text Vectorization\n",
        "\n",
        "Overview:\n",
        "\n",
        "This assignment involves applying preprocessing techniques and classical vectorization methods to the SMS Spam Dataset. Students will build a preprocessing pipeline, vectorize the text using various methods, and evaluate the impact on a classification task (spam vs. ham).\n",
        "In addition to vectorization, students must analyze the sparsity and feature space dimensions of their models to evaluate computational efficiency\n",
        "\n",
        "Submission Requirements   \n",
        "\t•\tCode: Submit a single Jupyter Notebook containing all the Python code.    \n",
        "\t•\tAnalysis: Include written responses to all analytical prompts in markdown cells within the notebook.    \n",
        "\t•\tVisualization: Include all required plots and charts in the notebook.   \n",
        "\t•\tFilename: Name the file as\n",
        "\t\t\tWeek2_Textvectorization_<YourName>.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "eL153Z8gTO2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL for the SMS Spam Dataset\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/sms.tsv\"\n",
        "\n",
        "# Load the dataset\n",
        "sms_data = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"First 5 rows of the SMS Spam Dataset:\")\n",
        "print(sms_data.head())\n",
        "\n",
        "# Check dataset size\n",
        "print(\"\\nDataset Size:\", sms_data.shape)\n",
        "\n",
        "# Split dataset into spam and ham categories for exploration\n",
        "print(\"\\nLabel Distribution:\")\n",
        "print(sms_data['label'].value_counts())"
      ],
      "metadata": {
        "id": "9VS3f3wM-fYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: preprocessing (20 points)\n",
        "\n",
        "\n",
        "\t1.\tTask:   \n",
        "\t•\tPreprocess the SMS Spam dataset:\n",
        "\t\t\t•\tTokenize the text.\n",
        "\t\t\t•\tApply stemming (Porter or Snowball) and lemmatization.\n",
        "\t\t\t•\tRemove special characters, numbers, and URLs.\n",
        "\t•\tCreate a reusable preprocessing pipeline.\n",
        "\t•\tJustify whether stemming or lemmatization is more appropriate for maintaining the semantic integrity of SMS-style language (e.g., 'u', 'r', 'gr8').\n",
        "\n",
        "\t2.\tDeliverable (15 points):\n",
        "\t•\tPython code implementing the preprocessing pipeline\n",
        "\t•\tExample output showing raw and preprocessed text.\n",
        "\n",
        "\t3.\tQuestion (5 points)\n",
        "\t•\tBeyond vocabulary size, how does the choice between stemming and lemmatization affect the sparsity of the resulting vectors in Part 2?"
      ],
      "metadata": {
        "id": "6gCLHGGqT2D6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Vectorization (40 points)\n",
        "\n",
        "\n",
        "\t1.\tTask:\n",
        "\t•\tVectorize the preprocessed text using:\n",
        "\t\t\t•\tBag of Words (BoW).\n",
        "\t\t\t•\tTF-IDF.\n",
        "\t\t\t•\tN-Grams (unigrams, bigrams, and trigrams).\n",
        "\t•\tCompare vector sizes and sparsity across methods\n",
        "\t•\tCalculate the total number of unique features generated for unigrams, bigrams, and trigrams separately.\n",
        "\n",
        "\t2.\tDeliverable (30 points):\n",
        "\t•\tPython code for vectorization using each method.\n",
        "\t•\tTable summarizing vector dimensions and sparsity.\n",
        "\n",
        "\t3.\tQuestion (10 points):\n",
        "\t•\tWhat are the trade-offs between Bag of Words and TF-IDF in terms of dimensionality and interpretability?\n",
        "\t•\tBased on the dimensionality of your N-gram vectors, discuss the 'curse of dimensionality. At what point does the benefit of context-awareness (e.g., trigrams) get outweighed by the computational cost?"
      ],
      "metadata": {
        "id": "G9ekGc0FUIht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: ML Classification Model (40 points)\n",
        "\n",
        "\n",
        "\t1.\tTask:\n",
        "\t•\tBuild a Support two Vector Machine (SVM) classifiers: one using **BoW vectors** and one using T**F-IDF vectors**\n",
        "\t•\tTrain the model to classify messages as spam or ham.\n",
        "\t•\tEvaluate the model using accuracy, precision, recall, and F1-score.\n",
        "\n",
        "\t2.\tDeliverable (30 points):\n",
        "\t•\tPython code for training and evaluating the model\n",
        "\t•\tClassification report summarizing model performance\n",
        "\t•\tProvide a side-by-side comparison table of Accuracy, Precision, Recall, and F1-score for both models\n",
        "\n",
        "\t3.\tQuestion (10 points):\n",
        "\t•\tTF-IDF is intended to prioritize rare but informative terms. Did the TF-IDF model outperform BoW for the 'Spam' class specifically? Why or why not?"
      ],
      "metadata": {
        "id": "qQL205b1UO6Z"
      }
    }
  ]
}