{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries and Dataset"
      ],
      "metadata": {
        "id": "fg8Xm0hvLmxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import reuters\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample text\n",
        "text = reuters.raw(reuters.fileids()[0])[:500]  # Extract the first 500 characters for moderate complexity\n",
        "\n",
        "# Display the sample text\n",
        "print(\"Sample Text:\")\n",
        "print(text)"
      ],
      "metadata": {
        "id": "cey_aKTpmDmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "jCxylKpO2762"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Tokenization with NLTK"
      ],
      "metadata": {
        "id": "u92EPdTyy9XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Tokenization with NLTK ===\")\n",
        "sentences_nltk = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization (NLTK):\", sentences_nltk)\n",
        "\n",
        "words_nltk = word_tokenize(text)\n",
        "print(\"\\nWord Tokenization (NLTK):\", words_nltk)"
      ],
      "metadata": {
        "id": "VbnStuCPy1fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Tokenization with SpaCy\n",
        "\n"
      ],
      "metadata": {
        "id": "nF8xEOeuL6XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Tokenization with SpaCy ===\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "tokens_spacy = [token.text for token in doc]\n",
        "print(\"Word Tokenization (SpaCy):\", tokens_spacy)"
      ],
      "metadata": {
        "id": "kmoJt1vll6je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Custom Tokenization Using Regular Expressions\n"
      ],
      "metadata": {
        "id": "M-ZUNJRbMvtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Custom Tokenization with Regex ===\")\n",
        "def custom_tokenizer_regex(text):\n",
        "    return re.findall(r\"\\b\\w+(?:-\\w+)*(?:'t|n't|'re|'ve|'ll|'d|'s)?\\b\", text)\n",
        "\n",
        "tokens_regex = custom_tokenizer_regex(text)\n",
        "print(\"Word Tokenization (Regex):\", tokens_regex)"
      ],
      "metadata": {
        "id": "5VbtwGMUnkyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Reflection Questions\n",
        "1. How do the tokenization outputs from NLTK, SpaCy, and Regex differ?\"\n",
        "2. Which method better handles edge cases like contractions or special characters? Provide examples."
      ],
      "metadata": {
        "id": "mJGQGMrYM3XL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words Removal\n",
        "#### Learning Goals:\n",
        "##### Understand the role of stop words in text preprocessing.\n",
        "##### Explore stop word removal using NLTK and SpaCy.\n",
        "##### Customize stop word lists for domain-specific tasks.\n",
        "##### Reflect on how stop word removal affects text analysis."
      ],
      "metadata": {
        "id": "Z17ljcFmPgcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Stop Word Removal with NLTK"
      ],
      "metadata": {
        "id": "qQRla78JQC27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Default stop words\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the Reuters sample text\n",
        "words_nltk = word_tokenize(text)\n",
        "\n",
        "# Remove stop words\n",
        "filtered_words_nltk = [word for word in words_nltk if word.lower() not in stop_words_nltk]\n",
        "print(\"Original Words (NLTK):\", words_nltk[:20])\n",
        "print(\"\\nFiltered Words (No Stop Words, NLTK):\", filtered_words_nltk[:20])"
      ],
      "metadata": {
        "id": "L6Be24hsQHXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Stop Word Removal with SpaCy"
      ],
      "metadata": {
        "id": "-XehUcoLQP2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "filtered_words_spacy = [token.text for token in doc if not token.is_stop]\n",
        "print(\"\\nFiltered Words (No Stop Words, SpaCy):\", filtered_words_spacy[:20])"
      ],
      "metadata": {
        "id": "ldZA3WcSQTYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Custom Stop Words"
      ],
      "metadata": {
        "id": "yfLx7Ag4Qb-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add domain-specific stop words\n",
        "custom_stop_words = stop_words_nltk.union({\"reuters\", \"said\", \"mr\"})\n",
        "filtered_words_custom = [word for word in words_nltk if word.lower() not in custom_stop_words]\n",
        "print(\"\\nFiltered Words (Custom Stop Words):\", filtered_words_custom[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGvmrUz5QdGL",
        "outputId": "456cff30-d41d-46ba-ec24-d54fdf3ea161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filtered Words (Custom Stop Words): ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'U.S.-JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'U.S.', 'Japan', 'raised', 'fears', 'among', 'many', 'Asia', \"'s\", 'exporting', 'nations', 'row']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflection Questions\n",
        "1. How do the filtered words from NLTK and SpaCy differ?\n",
        "2. Why might you want to customize stop words for domain-specific tasks? Provide examples.\""
      ],
      "metadata": {
        "id": "7vy2ltxj4TDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Normalization\n",
        "#### Learning Goals:\n",
        "##### Focus on lowercasing, punctuation removal and normalization of text inconsistencies"
      ],
      "metadata": {
        "id": "8cmGbfJQRasl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Lowercasing Text"
      ],
      "metadata": {
        "id": "4s3DGQlERykl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lowercased_text = text.lower()\n",
        "print(\"Original Text (First 200 characters):\")\n",
        "print(text[:200])\n",
        "print(\"\\nLowercased Text (First 200 characters):\")\n",
        "print(lowercased_text[:200])"
      ],
      "metadata": {
        "id": "a7W9E9ZDR2Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Removing Punctuation"
      ],
      "metadata": {
        "id": "WUxgD_QiR-uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "print(\"\\n=== Removing Punctuation ===\")\n",
        "# Remove punctuation\n",
        "text_no_punctuation = ''.join([char for char in lowercased_text if char not in string.punctuation])\n",
        "print(\"Text without Punctuation (First 200 characters):\")\n",
        "print(text_no_punctuation[:200])"
      ],
      "metadata": {
        "id": "sEHKtf_QSC1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Handling Numbers"
      ],
      "metadata": {
        "id": "Y4-HCgChSLrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Removing Numbers ===\")\n",
        "# Remove numbers\n",
        "text_no_numbers = ''.join([char for char in text_no_punctuation if not char.isdigit()])\n",
        "print(\"Text without Numbers (First 200 characters):\")\n",
        "print(text_no_numbers[:200])"
      ],
      "metadata": {
        "id": "XQTq7PbXSPeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Normalizing Text Inconsistencies"
      ],
      "metadata": {
        "id": "ije6SlkGSaTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Normalizing Text Inconsistencies (British to American English) ===\")\n",
        "# Define a function for normalizing British to American English\n",
        "def normalize_british_to_american(text):\n",
        "    british_to_american = {\n",
        "        \"colour\": \"color\",\n",
        "        \"favourite\": \"favorite\",\n",
        "        \"organise\": \"organize\",\n",
        "        \"realise\": \"realize\"\n",
        "    }\n",
        "    words = text.split()\n",
        "    normalized_words = [british_to_american.get(word, word) for word in words]\n",
        "    return ' '.join(normalized_words)\n",
        "\n",
        "# Sample text with British spellings\n",
        "british_text = \"I organise my tasks and realise my favourite colour is blue.\"\n",
        "normalized_text = normalize_british_to_american(british_text)\n",
        "print(\"Original Text (British):\", british_text)\n",
        "print(\"Normalized Text (American):\", normalized_text)\n",
        "\n",
        "# Reflection Questions\n",
        "print(\"\\n=== Reflection Questions ===\")\n",
        "print(\"1. Why might lowercasing or punctuation removal be inappropriate for certain NLP tasks?\")\n",
        "print(\"2. How does normalizing text inconsistencies (e.g., British vs. American spellings) improve data quality?\")"
      ],
      "metadata": {
        "id": "h32vZuUXSVEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why might lowercasing or punctuation removal be inappropriate for certain NLP tasks (e.g., sentiment analysis or named entity recognition)?\n",
        "\n",
        "2. What are the benefits of normalizing British to American spellings in multilingual datasets?\n",
        "3. Can you think of a domain where preserving numbers is critical? Provide examples.‚Äù"
      ],
      "metadata": {
        "id": "J7D4UWCsShZU"
      }
    }
  ]
}