{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg8Xm0hvLmxj"
      },
      "source": [
        "# Import libraries and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cey_aKTpmDmC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to\n",
            "[nltk_data]     C:\\Users\\water\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\water\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Text:\n",
            "ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
            "  Mounting trade friction between the\n",
            "  U.S. And Japan has raised fears among many of Asia's exporting\n",
            "  nations that the row could inflict far-reaching economic\n",
            "  damage, businessmen and officials said.\n",
            "      They told Reuter correspondents in Asian capitals a U.S.\n",
            "  Move against Japan might boost protectionist sentiment in the\n",
            "  U.S. And lead to curbs on American imports of their products.\n",
            "      But some exporters said that while the conflict wo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import reuters\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample text\n",
        "text = reuters.raw(reuters.fileids()[0])[:500]  # Extract the first 500 characters for moderate complexity\n",
        "\n",
        "# Display the sample text\n",
        "print(\"Sample Text:\")\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCxylKpO2762"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u92EPdTyy9XW"
      },
      "source": [
        "### Part 1: Tokenization with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbnStuCPy1fs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Tokenization with NLTK ===\n",
            "Sentence Tokenization (NLTK): [\"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia's exporting\\n  nations that the row could inflict far-reaching economic\\n  damage, businessmen and officials said.\", 'They told Reuter correspondents in Asian capitals a U.S.\\n  Move against Japan might boost protectionist sentiment in the\\n  U.S. And lead to curbs on American imports of their products.', 'But some exporters said that while the conflict wo']\n",
            "\n",
            "Word Tokenization (NLTK): ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U.S.-JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U.S.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'s\", 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far-reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.', 'They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U.S.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U.S.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.', 'But', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'wo']\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Tokenization with NLTK ===\")\n",
        "sentences_nltk = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization (NLTK):\", sentences_nltk)\n",
        "\n",
        "words_nltk = word_tokenize(text)\n",
        "print(\"\\nWord Tokenization (NLTK):\", words_nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF8xEOeuL6XS"
      },
      "source": [
        "### Part 2: Tokenization with SpaCy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmoJt1vll6je"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Tokenization with SpaCy ===\n",
            "Word Tokenization (SpaCy): ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U.S.-JAPAN', 'RIFT', '\\n  ', 'Mounting', 'trade', 'friction', 'between', 'the', '\\n  ', 'U.S.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'s\", 'exporting', '\\n  ', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', '\\n  ', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.', '\\n      ', 'They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U.S.', '\\n  ', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', '\\n  ', 'U.S.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.', '\\n      ', 'But', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'wo']\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Tokenization with SpaCy ===\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "tokens_spacy = [token.text for token in doc]\n",
        "print(\"Word Tokenization (SpaCy):\", tokens_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-ZUNJRbMvtk"
      },
      "source": [
        "### Part 3: Custom Tokenization Using Regular Expressions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VbtwGMUnkyc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Custom Tokenization with Regex ===\n",
            "Word Tokenization (Regex): ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', 'S', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', 'S', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', \"Asia's\", 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far-reaching', 'economic', 'damage', 'businessmen', 'and', 'officials', 'said', 'They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U', 'S', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U', 'S', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', 'But', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'wo']\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Custom Tokenization with Regex ===\")\n",
        "def custom_tokenizer_regex(text):\n",
        "    return re.findall(r\"\\b\\w+(?:-\\w+)*(?:'t|n't|'re|'ve|'ll|'d|'s)?\\b\", text)\n",
        "\n",
        "tokens_regex = custom_tokenizer_regex(text)\n",
        "print(\"Word Tokenization (Regex):\", tokens_regex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for token in tokens_regex:\n",
        "    if \"'\" in token:\n",
        "        print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJGQGMrYM3XL"
      },
      "source": [
        " Reflection Questions\n",
        "1. How do the tokenization outputs from NLTK, SpaCy, and Regex differ?\"\n",
        "2. Which method better handles edge cases like contractions or special characters? Provide examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z17ljcFmPgcl"
      },
      "source": [
        "# Stop Words Removal\n",
        "#### Learning Goals:\n",
        "##### Understand the role of stop words in text preprocessing.\n",
        "##### Explore stop word removal using NLTK and SpaCy.\n",
        "##### Customize stop word lists for domain-specific tasks.\n",
        "##### Reflect on how stop word removal affects text analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQRla78JQC27"
      },
      "source": [
        "### Part 1: Stop Word Removal with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6Be24hsQHXd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Words (NLTK): ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U.S.-JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U.S.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many']\n",
            "\n",
            "Filtered Words (No Stop Words, NLTK): ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'U.S.-JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'U.S.', 'Japan', 'raised', 'fears', 'among', 'many', 'Asia', \"'s\", 'exporting', 'nations', 'row']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\water\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Default stop words\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the Reuters sample text\n",
        "words_nltk = word_tokenize(text)\n",
        "\n",
        "# Remove stop words\n",
        "filtered_words_nltk = [word for word in words_nltk if word.lower() not in stop_words_nltk]\n",
        "print(\"Original Words (NLTK):\", words_nltk[:20])\n",
        "print(\"\\nFiltered Words (No Stop Words, NLTK):\", filtered_words_nltk[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XehUcoLQP2H"
      },
      "source": [
        "### Part 2: Stop Word Removal with SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldZA3WcSQTYo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Filtered Words (No Stop Words, SpaCy): ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'U.S.-JAPAN', 'RIFT', '\\n  ', 'Mounting', 'trade', 'friction', '\\n  ', 'U.S.', 'Japan', 'raised', 'fears', 'Asia', 'exporting', '\\n  ', 'nations', 'row']\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(text)\n",
        "filtered_words_spacy = [token.text for token in doc if not token.is_stop]\n",
        "print(\"\\nFiltered Words (No Stop Words, SpaCy):\", filtered_words_spacy[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfLx7Ag4Qb-X"
      },
      "source": [
        "### Part 3: Custom Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGvmrUz5QdGL",
        "outputId": "456cff30-d41d-46ba-ec24-d54fdf3ea161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Filtered Words (Custom Stop Words): ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'U.S.-JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'U.S.', 'Japan', 'raised', 'fears', 'among', 'many', 'Asia', \"'s\", 'exporting', 'nations', 'row']\n"
          ]
        }
      ],
      "source": [
        "# Add domain-specific stop words\n",
        "custom_stop_words = stop_words_nltk.union({\"reuters\", \"said\", \"mr\"})\n",
        "filtered_words_custom = [word for word in words_nltk if word.lower() not in custom_stop_words]\n",
        "print(\"\\nFiltered Words (Custom Stop Words):\", filtered_words_custom[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vy2ltxj4TDF"
      },
      "source": [
        "# Reflection Questions\n",
        "1. How do the filtered words from NLTK and SpaCy differ?\n",
        "2. Why might you want to customize stop words for domain-specific tasks? Provide examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cmGbfJQRasl"
      },
      "source": [
        "# Text Normalization\n",
        "#### Learning Goals:\n",
        "##### Focus on lowercasing, punctuation removal and normalization of text inconsistencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s3DGQlERykl"
      },
      "source": [
        "### Part 1: Lowercasing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7W9E9ZDR2Zh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text (First 200 characters):\n",
            "ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
            "  Mounting trade friction between the\n",
            "  U.S. And Japan has raised fears among many of Asia's exporting\n",
            "  nations that the row could inflict far-reachin\n",
            "\n",
            "Lowercased Text (First 200 characters):\n",
            "asian exporters fear damage from u.s.-japan rift\n",
            "  mounting trade friction between the\n",
            "  u.s. and japan has raised fears among many of asia's exporting\n",
            "  nations that the row could inflict far-reachin\n"
          ]
        }
      ],
      "source": [
        "lowercased_text = text.lower()\n",
        "print(\"Original Text (First 200 characters):\")\n",
        "print(text[:200])\n",
        "print(\"\\nLowercased Text (First 200 characters):\")\n",
        "print(lowercased_text[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUxgD_QiR-uR"
      },
      "source": [
        "### Part 2: Removing Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEHKtf_QSC1g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Removing Punctuation ===\n",
            "Text without Punctuation (First 200 characters):\n",
            "asian exporters fear damage from usjapan rift\n",
            "  mounting trade friction between the\n",
            "  us and japan has raised fears among many of asias exporting\n",
            "  nations that the row could inflict farreaching econo\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "print(\"\\n=== Removing Punctuation ===\")\n",
        "# Remove punctuation\n",
        "text_no_punctuation = ''.join([char for char in lowercased_text if char not in string.punctuation])\n",
        "print(\"Text without Punctuation (First 200 characters):\")\n",
        "print(text_no_punctuation[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-HCgChSLrv"
      },
      "source": [
        "### Part 3: Handling Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQTq7PbXSPeT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Removing Numbers ===\n",
            "Text without Numbers (First 200 characters):\n",
            "asian exporters fear damage from usjapan rift\n",
            "  mounting trade friction between the\n",
            "  us and japan has raised fears among many of asias exporting\n",
            "  nations that the row could inflict farreaching econo\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Removing Numbers ===\")\n",
        "# Remove numbers\n",
        "text_no_numbers = ''.join([char for char in text_no_punctuation if not char.isdigit()])\n",
        "print(\"Text without Numbers (First 200 characters):\")\n",
        "print(text_no_numbers[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ije6SlkGSaTj"
      },
      "source": [
        "### Part 4: Normalizing Text Inconsistencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h32vZuUXSVEF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Normalizing Text Inconsistencies (British to American English) ===\n",
            "Original Text (British): I organise my tasks and realise my favourite colour is blue.\n",
            "Normalized Text (American): I organize my tasks and realize my favorite color is blue.\n",
            "\n",
            "=== Reflection Questions ===\n",
            "1. Why might lowercasing or punctuation removal be inappropriate for certain NLP tasks?\n",
            "2. How does normalizing text inconsistencies (e.g., British vs. American spellings) improve data quality?\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Normalizing Text Inconsistencies (British to American English) ===\")\n",
        "# Define a function for normalizing British to American English\n",
        "def normalize_british_to_american(text):\n",
        "    british_to_american = {\n",
        "        \"colour\": \"color\",\n",
        "        \"favourite\": \"favorite\",\n",
        "        \"organise\": \"organize\",\n",
        "        \"realise\": \"realize\"\n",
        "    }\n",
        "    words = text.split()\n",
        "    normalized_words = [british_to_american.get(word, word) for word in words]\n",
        "    return ' '.join(normalized_words)\n",
        "\n",
        "# Sample text with British spellings\n",
        "british_text = \"I organise my tasks and realise my favourite colour is blue.\"\n",
        "normalized_text = normalize_british_to_american(british_text)\n",
        "print(\"Original Text (British):\", british_text)\n",
        "print(\"Normalized Text (American):\", normalized_text)\n",
        "\n",
        "# Reflection Questions\n",
        "print(\"\\n=== Reflection Questions ===\")\n",
        "print(\"1. Why might lowercasing or punctuation removal be inappropriate for certain NLP tasks?\")\n",
        "print(\"2. How does normalizing text inconsistencies (e.g., British vs. American spellings) improve data quality?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7D4UWCsShZU"
      },
      "source": [
        "1. Why might lowercasing or punctuation removal be inappropriate for certain NLP tasks (e.g., sentiment analysis or named entity recognition)?\n",
        "\n",
        "2. What are the benefits of normalizing British to American spellings in multilingual datasets?\n",
        "3. Can you think of a domain where preserving numbers is critical? Provide examples.‚Äù"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DSE6212",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
