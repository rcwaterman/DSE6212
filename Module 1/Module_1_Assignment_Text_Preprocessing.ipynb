{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 1 Assignment: Text Preprocessing\n",
        "\n",
        "Overview:\n",
        "\n",
        "This assignment requires applying the concepts of tokenization, stop word removal, and text normalization.\n",
        "\n",
        "Students will write their own code for the tasks, analyze the results, and respond to exercises based on results.\n",
        "\n",
        "Submission Requirements   \n",
        "\t•\tCode: Submit a single Jupyter Notebook containing all the Python code.    \n",
        "\t•\tAnalysis: Include written responses to all analytical prompts in markdown cells within the notebook.    \n",
        "\t•\tVisualization: Include all required plots and charts in the notebook.   \n",
        "\t•\tFilename: Name the file as\n",
        "\t\t\tWeek1_TextPreprocessing_<YourName>.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "eL153Z8gTO2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL for the SMS Spam Dataset\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/sms.tsv\"\n",
        "\n",
        "# Load the dataset\n",
        "sms_data = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"First 5 rows of the SMS Spam Dataset:\")\n",
        "print(sms_data.head())\n",
        "\n",
        "# Check dataset size\n",
        "print(\"\\nDataset Size:\", sms_data.shape)\n",
        "\n",
        "# Split dataset into spam and ham categories for exploration\n",
        "print(\"\\nLabel Distribution:\")\n",
        "print(sms_data['label'].value_counts())"
      ],
      "metadata": {
        "id": "9VS3f3wM-fYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Tokenization (20 points)\n",
        "\n",
        "\n",
        "\t1.\tTask:   \n",
        "\t•\tLoad the SMS Spam dataset and select the first 10 messages for tokenization.  \n",
        "\t•\tTokensize using\n",
        "\t\t\t- NLTK\n",
        "\t\t\t- SpaCy\n",
        "\t\t\t- Cutom Regex based tokenizer\n",
        "\t•\tWrite a custom tokenization function using regular expressions to handle special cases like contractions and hyphenated words.\n",
        "\n",
        "\t2.\tDeliverable (15 points):\n",
        "\t•\tPython script that demonstrates tokenization using all three approaches.\n",
        "\t•\tA brief written response comparing the outputs and discussing differences (e.g., handling contractions, special characters, and hyphenated words).\n",
        "\n",
        "\t3.\tQuestion (5 points)\n",
        "\t•\tHow do the tokenization results differ between NLTK, SpaCy, and Regex? Which method would you prefer for preprocessing SMS messages, and why?"
      ],
      "metadata": {
        "id": "6gCLHGGqT2D6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Stop Word Removal (20 points)\n",
        "\n",
        "\n",
        "\t1.\tTask:\n",
        "\t•\tUse NLTK & SpaCy to remove default stop words from the tokenized messages.\n",
        "\t•\tAdd custom stop words relevant to SMS messages (e.g., “free”, “click”, “call”).\n",
        "\n",
        "\t2.\tDeliverable (15 points):\n",
        "\t•\tPython script for stop word removal using NLTK 7 SpCy\n",
        "\t•\tA bar chart comparing word frequency distributions before and after stop word removal.\n",
        "\n",
        "\t3.\tQuestion (5 points):\n",
        "\t•\tHow does removing stop words impact the vocabulary size and word frequency distribution in the SMS dataset?"
      ],
      "metadata": {
        "id": "G9ekGc0FUIht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Text Normalization (30 points)\n",
        "\n",
        "\n",
        "\t1.\tTask:\n",
        "\t•\tNormalize the SMS messages by:\n",
        "\t•\tLowercasing all text\n",
        "\t•\tRemoving punctuation and numbers.\n",
        "\t•\tReplacing abbreviations (e.g., “u” → “you”, “r” → “are”).\n",
        "\t•\tImplement a reusable normalization function that combines these steps.\n",
        "\n",
        "\t2.\tDeliverable (25 points):\n",
        "\t•\tPython code for the normalization function.\n",
        "\t•\tExample output showing the text before and after normalization.\n",
        "\n",
        "\t3.\tQuestion (5 points):\n",
        "\t•\tWhich normalization steps had the most significant impact on SMS messages? Explain why."
      ],
      "metadata": {
        "id": "qQL205b1UO6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Combining All Preprocessing Steps (30 points)\n",
        "\n",
        "\t1.\tTask:\n",
        "\t•\tCombine tokenization, stop word removal, and normalization into a single preprocessing pipeline.\n",
        "\t•\tApply the pipeline to first 50 messages in the SMS Spam dataset\n",
        "\t•\tVisualize the most frequent words after preprocessing using a bar chart\n",
        "\n",
        "\t2.\tDeliverable (25 points):\n",
        "\t•\tPython code for the preprocessing pipeline.\n",
        "\t•\tA bar chart idplaying the top 10 most frequent words.\n",
        "\n",
        "\t3.\tQuestion (5 points):\n",
        "\t•\tHow does preprocessing improve the dataset for text classification tasks (e.g. spam detection)?"
      ],
      "metadata": {
        "id": "Fy5jNwRJUTda"
      }
    }
  ]
}